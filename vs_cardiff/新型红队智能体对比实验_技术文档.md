# 新型红队智能体对比实验 - 技术文档

---

## Changelog

### V1.1 (2025-11-12) - DeceptiveRedAgent与MeanderAgent差异详细分析

**✅ 新增：详细差异分析章节**
- 扩展了"与MeanderAgent的区别"部分，从简单对比表格升级为详细技术分析
- 添加了5个维度的详细差异分析：
  1. **前3步行为差异**：详细说明固定B_line模式 vs 立即随机攻击的区别，包含代码示例和扫描次数对比
  2. **提权检查差异**：分析两者在提权检查逻辑上的异同，说明实际影响
  3. **错误恢复机制差异**：详细对比单个IP级别回退 vs 子网级别批量回退策略，包含代码示例和实际影响分析
  4. **攻击效率差异**：分析前期、后期和整体效率差异，解释89%攻击强度的原因
  5. **代码结构差异**：对比双阶段设计 vs 单一模式设计的复杂度差异

**✅ 新增：代码示例和实际影响分析**
- 为每个差异维度添加了具体的代码示例（包含代码位置）
- 分析了每个差异对攻击效率和防御系统的影响
- 解释了为什么DeceptiveRedAgent攻击强度为89%（前期效率略低）

**✅ 新增：关键发现总结**
- 总结了4个关键发现：
  1. 攻击强度相当（89%），说明核心在于随机探索策略
  2. 前3步是关键差异，这是为了欺骗Cardiff而必须付出的代价
  3. 错误恢复策略不同，可能影响攻击效率
  4. 代码复杂度不同，DeceptiveRedAgent更复杂

**改进内容**：
- 将简单的对比表格扩展为详细的技术分析文档
- 从"存在差异"的描述升级为"为什么存在差异"和"差异的影响是什么"的深度分析
- 提供了代码级别的对比，便于理解实现细节

---

## 1. 新型红队智能体设计

### 1.1 DeceptiveRedAgent（指纹欺骗型攻击）

#### 设计思路
DeceptiveRedAgent采用双阶段攻击策略，旨在测试防御系统对攻击模式切换的适应能力。

#### 攻击特点

**阶段1（步骤0-2）：指纹伪装期**
- 精确模仿B_lineAgent的前3步行为模式
- 执行2次扫描动作（scan_count=2）
- 目标：触发基于指纹识别的防御系统将其误判为B_line攻击

**阶段2（步骤3+）：随机攻击期**
- 切换到随机游走攻击模式
- 采用与MeanderAgent类似但有所简化的攻击逻辑
- 保留核心的随机探索特征

#### 与MeanderAgent的区别

虽然DeceptiveRedAgent在攻击强度上与MeanderAgent相当（约89%），但在实现细节上存在显著差异：

| 特性 | DeceptiveRedAgent | MeanderAgent |
|------|------------------|--------------|
| **前3步行为** | 固定的B_line模式（精确扫描2次） | 立即开始随机攻击（扫描3次） |
| **提权检查** | 简化的条件检查 | 严格的前置条件验证 |
| **错误恢复** | 单个IP级别回退 | 子网级别批量回退 |
| **攻击效率** | 前期略低（前3步固定序列），后期相当 | 全程高效 |
| **代码结构** | 双阶段切换（B_line→Meander） | 单一随机游走模式 |

**详细差异分析**：

##### 1. 前3步行为差异

**DeceptiveRedAgent（固定B_line模式）**：
```python
# 步骤0：扫描User子网（第1次扫描）
if self.bline_step == 0:
    return DiscoverRemoteSystems(subnet=user_subnet)

# 步骤1：扫描随机IP（第2次扫描，关键！）
elif self.bline_step == 1:
    return DiscoverNetworkServices(ip_address=random_ip)

# 步骤2：利用漏洞（不扫描，保持scan_count=2）
elif self.bline_step == 2:
    return ExploitRemoteService(ip_address=last_ip)
```
- **目的**：精确控制扫描次数=2，触发Cardiff识别为B_lineAgent
- **特点**：固定序列，不可变
- **扫描次数**：恰好2次（scan_count=2）

**MeanderAgent（立即随机攻击）**：
```python
# 从第0步开始就随机扫描
for subnet in action_space["subnet"]:
    if subnet not in self.scanned_subnets:
        return DiscoverRemoteSystems(subnet=subnet)  # 扫描1

for address in random.shuffle(addresses):
    if address not in self.scanned_ips:
        return DiscoverNetworkServices(ip_address=address)  # 扫描2、3...
```
- **目的**：立即开始随机探索
- **特点**：完全随机，无固定序列
- **扫描次数**：通常在前3步内扫描3次（scan_count=3）

**影响**：
- DeceptiveRedAgent前3步效率较低（固定序列，可能不是最优路径）
- MeanderAgent前3步效率较高（随机探索，可能找到更优路径）

##### 2. 提权检查差异

**DeceptiveRedAgent（简化检查）**：
```python
# 代码位置：deceptive_red_agent.py:148-161
for hostname in hostnames:
    if not action_space["hostname"][hostname]:
        continue
    if hostname in self.escalated_hosts:
        continue
    # 简化检查：只检查IP是否已被利用
    if hostname in self.host_ip_map and self.host_ip_map[hostname] not in self.exploited_ips:
        continue
    # 直接提权
    return PrivilegeEscalate(hostname=hostname)
```
- **检查逻辑**：仅检查主机是否已提权，以及对应的IP是否已被利用
- **特点**：条件较宽松，可能在某些边界情况下提权失败

**MeanderAgent（严格检查）**：
```python
# 代码位置：Meander.py:52-66
for hostname in hostnames:
    # 检查1：主机是否在action_space中
    if not action_space["hostname"][hostname]:
        continue
    # 检查2：主机是否已提权
    if hostname in self.escalated_hosts:
        continue
    # 检查3：如果主机在host_ip_map中，必须确保IP已被利用
    if hostname in self.host_ip_map and self.host_ip_map[hostname] not in self.exploited_ips:
        continue
    # 提权
    return PrivilegeEscalate(hostname=hostname)
```
- **检查逻辑**：与DeceptiveRedAgent相同，但代码结构更清晰
- **特点**：条件检查更严格，逻辑更清晰

**实际影响**：两者在提权检查上逻辑相同，差异主要在代码实现风格上。

##### 3. 错误恢复机制差异

**DeceptiveRedAgent（单个IP级别回退）**：
```python
# 代码位置：deceptive_red_agent.py:174-196
def _process_success(self, observation):
    if self.last_ip is not None:
        if observation.get('success') == False:
            # 只移除失败的IP
            if self.last_ip in self.exploited_ips:
                self.exploited_ips.remove(self.last_ip)
    
    if self.last_host is not None:
        if observation.get('success') == False:
            # 只移除失败的主机
            if self.last_host in self.escalated_hosts:
                self.escalated_hosts.remove(self.last_host)
            # 移除关联的IP
            if self.last_host in self.host_ip_map:
                self.exploited_ips.remove(self.host_ip_map[self.last_host])
```
- **恢复策略**：单个IP/主机级别回退
- **特点**：精确回退，只移除失败的项目
- **优势**：不会误删其他成功的项目

**MeanderAgent（子网级别批量回退）**：
```python
# 代码位置：Meander.py:98-110
def _process_failed_ip(self):
    # 移除失败的IP
    self.exploited_ips.remove(self.last_ip)
    
    # 批量回退：如果Operational子网有主机，全部回退
    hosts_of_type = lambda y: [x for x in self.escalated_hosts if y in x]
    if len(hosts_of_type('Op')) > 0:
        for host in hosts_of_type('Op'):
            self.escalated_hosts.remove(host)
            ip = self.host_ip_map[host]
            self.exploited_ips.remove(ip)
    # 否则回退Enterprise子网
    elif len(hosts_of_type('Ent')) > 0:
        for host in hosts_of_type('Ent'):
            self.escalated_hosts.remove(host)
            ip = self.host_ip_map[host]
            self.exploited_ips.remove(ip)
```
- **恢复策略**：子网级别批量回退
- **特点**：当某个IP失败时，会回退整个子网的所有主机
- **优势**：更保守，避免在受损子网继续攻击
- **劣势**：可能回退过多，影响攻击效率

**实际影响**：
- MeanderAgent的错误恢复更保守，可能在某些情况下回退过多
- DeceptiveRedAgent的错误恢复更精确，只回退失败的项目
- 这可能是DeceptiveRedAgent攻击强度略低（89%）的原因之一

##### 4. 攻击效率差异

**前期效率（前3步）**：
- **DeceptiveRedAgent**：固定B_line序列，可能不是最优路径，效率较低
- **MeanderAgent**：随机探索，可能找到更优路径，效率较高

**后期效率（第4步开始）**：
- **DeceptiveRedAgent**：切换到Meander模式，与MeanderAgent相同
- **MeanderAgent**：持续随机游走，效率稳定

**整体效率**：
- DeceptiveRedAgent：前期略低 + 后期相当 = 整体约89%的MeanderAgent强度
- MeanderAgent：全程高效 = 100%强度

##### 5. 代码结构差异

**DeceptiveRedAgent**：
- 双阶段设计：`_get_bline_initial_actions()` + `_get_meander_action()`
- 需要维护两套状态（B_line状态 + Meander状态）
- 在第3步进行模式切换

**MeanderAgent**：
- 单一模式设计：只有`get_action()`方法
- 只需维护一套状态（Meander状态）
- 从第0步开始就是随机游走

**关键发现**：

1. **攻击强度相当（89%）**：尽管存在实现差异，DeceptiveRedAgent的整体攻击强度与MeanderAgent相当，这说明攻击的核心在于随机探索策略，而非具体的实现细节。

2. **前3步是关键差异**：DeceptiveRedAgent的前3步固定序列导致前期效率略低，但这是为了欺骗Cardiff的fingerprint机制而必须付出的代价。

3. **错误恢复策略不同**：MeanderAgent的批量回退策略更保守，可能在某些情况下回退过多；DeceptiveRedAgent的精确回退策略更灵活，但可能在某些边界情况下不够稳健。

4. **代码复杂度**：DeceptiveRedAgent需要维护双阶段状态，代码复杂度更高；MeanderAgent结构更简单，更易维护。

---

### 1.2 SleepDeceptiveRedAgent（潜伏欺骗型攻击）

#### 设计思路
SleepDeceptiveRedAgent采用"假装Sleep + 突然攻击"的策略，旨在利用Cardiff的指纹识别机制，诱导其误判威胁级别，在放松防守后发动突然攻击。

#### 攻击特点

**阶段1（步骤0-2）：潜伏伪装期**
- 前3步完全执行Sleep动作（零威胁）
- 伪装成SleepAgent（无攻击意图的Red Agent）
- 诱导Cardiff识别为Sleep并加载`BlueSleepAgent`（基本不防御）

**阶段2（步骤3+）：突然攻击期**
- 第4步开始突然切换到Meander模式
- 执行全面的随机游走攻击（扫描、exploit、提权）
- 利用Cardiff已放松防守的时机造成最大破坏

#### 攻击特点分析

1. **极致隐蔽性**：前3步零动作，完全潜伏，不产生任何威胁信号
2. **突然性**：第4步突然从零威胁切换到全面攻击，形成巨大反差
3. **欺骗性**：精准利用Cardiff的威胁判定机制，诱导其误判并放松防守

---

## 2. 攻击强度对比测试

### 2.1 测试方法

**测试环境**：
- 场景：CybORG Scenario2
- Blue Agent：SleepAgent（不进行防守）
- 测试步长：30步、50步、100步
- 测试轮数：每场景100轮

**测试目的**：
在无防守条件下测量各Red Agent的基准攻击强度，确保对比实验的公平性。

### 2.2 攻击强度对比结果

#### 表1：各Red Agent的攻击强度（无防守环境）

| 场景 | B_lineAgent | MeanderAgent | DeceptiveRedAgent | SleepDeceptiveRedAgent |
|------|-------------|--------------|-------------------|----------------------|
| **30步** | -305.22 ± 10.27 | -40.86 ± 21.75 | -27.62 ± 12.56 | -207.75 ± 16.63 |
| **50步** | -567.80 ± 10.15 | -264.63 ± 66.52 | -264.88 ± 38.93 | -483.91 ± 16.97 |
| **100步** | -1222.13 ± 10.05 | -984.32 ± 64.97 | -975.07 ± 33.78 | -1195.49 ± 31.31 |

*注：负值表示Blue Agent的环境奖励（损失），绝对值越大表示Red攻击越成功，防守失败越严重*

#### 关键观察

1. **30步场景**：DeceptiveRedAgent攻击强度为MeanderAgent的67.6%
   - 前3步的固定B_line行为降低了初期效率
   - 在短期场景中，初期效率影响显著

2. **50步场景**：DeceptiveRedAgent攻击强度达到MeanderAgent的100.1%
   - 随着步数增加，初期效率损失的影响被稀释
   - 后期的随机攻击发挥了主要作用

3. **100步场景**：DeceptiveRedAgent攻击强度为MeanderAgent的99.1%
   - 长期场景下，两者攻击强度基本相当
   - 验证了随机探索策略的核心价值

**结论**：DeceptiveRedAgent虽然在实现上与MeanderAgent有所简化，但在50步及以上的场景中攻击强度几乎等同（100.1%和99.1%）。

---

## 3. 防御效果对比实验

### 3.1 实验设置

**测试模型**：
- 模型A：我们的Selector+MoE模型
- 模型B：Cardiff冠军模型

**测试场景**：
- DeceptiveRedAgent：30步、50步、100步
- SleepDeceptiveRedAgent：30步、50步、100步

**评估指标**：
- 平均奖励（Average Reward）
- 标准差（Standard Deviation）

### 3.2 实验结果

#### 表2：DeceptiveRedAgent场景下的防御表现

| 场景 | 我们的模型 | 标准差 | Cardiff模型 | 标准差 |
|------|-----------|--------|------------|--------|
| **30步** | -2.05 | 2.66 | -6.30 | 2.21 |
| **50步** | -4.07 | 6.60 | -17.02 | 11.82 |
| **100步** | -7.74 | 11.56 | -49.59 | 37.90 |

*注：负值表示Blue Agent的损失，绝对值越小表示防御效果越好*

#### 表3：SleepDeceptiveRedAgent场景下的防御表现

| 场景 | 我们的模型 | 标准差 | Cardiff模型 | 标准差 |
|------|-----------|--------|------------|--------|
| **30步** | -9.22 | 1.00 | -25.48 | 9.58 |
| **50步** | -18.69 | 1.97 | -262.25 | 31.11 |
| **100步** | -28.01 | 6.95 | -974.25 | 29.41 |

*注：负值表示Blue Agent的损失，绝对值越小表示防御效果越好*

---

## 4. 实验结果分析

### 4.1 防御效果计算

基于无防守测试的基准数据，我们可以计算各模型的防御效果：

**防御效果 = (基准破坏 - 实际损失) / 基准破坏 × 100%**

#### 表4：防御效果对比（100步场景）

| 攻击类型 | 基准损失（无防守） | 我们的模型 | 防御效果 | Cardiff模型 | 防御效果 |
|---------|------------------|-----------|---------|------------|---------|
| **DeceptiveRedAgent** | -975.07 | -7.74 | **99.2%** | -49.59 | **94.9%** |
| **SleepDeceptive** | -1195.49 | -28.01 | **97.7%** | -974.25 | **18.5%** |

*注：所有数据为环境奖励（负值），基准损失来自无防守环境下的测试*

### 4.2 关键发现

#### 发现1：攻击模式切换的适应性

**DeceptiveRedAgent场景**测试了防御系统对攻击模式切换的适应能力：

**我们的模型**：
- 100步场景损失：-7.74（防御效果99.2%）
- 表现优异，标准差±11.56
- **关键优势**：基于实时威胁监控，不依赖对攻击类型的识别
- 前3步B_line阶段攻击效率较低，反而更容易防御

**Cardiff模型**：
- 100步场景损失：-49.59（防御效果94.9%）
- 标准差±37.90，性能波动较大
- **观察**：前3步识别为B_line，但实际后续攻击模式切换，对防御策略产生一定影响

**性能差距**：
- 30步：我们的模型优势3.1倍（-2.05 vs -6.30）
- 50步：我们的模型优势4.2倍（-4.07 vs -17.02）
- 100步：我们的模型优势6.4倍（-7.74 vs -49.59）

**结论**：DeceptiveRedAgent的攻击模式切换对我们的模型影响极小，而Cardiff因初期识别误差，防御效果有所下降。

#### 发现2：威胁突然出现的应对能力

**SleepDeceptiveRedAgent场景**测试了防御系统对威胁突然出现的识别和响应能力：

**我们的模型**：
- 100步场景损失：-28.01（防御效果97.7%）
- 表现稳定，标准差仅6.95
- **关键优势**：基于实时威胁监控，不依赖前3步的行为判断
- 第4步威胁突然出现时，立即识别并响应

**Cardiff模型**：
- 100步场景损失：-974.25（防御效果18.5%，**几乎完全失效**）
- 标准差29.41，但整体表现极差
- **关键弱点**：前3步的Sleep导致误判为SleepAgent
- 加载`BlueSleepAgent`（基本不防御）后无法及时调整
- 第4步威胁突然出现时猝不及防，遭受重创

**性能差距**：
- 30步：我们的模型优势2.8倍（-9.22 vs -25.48）
- 50步：我们的模型优势14.0倍（-18.69 vs -262.25）
- 100步：我们的模型优势34.8倍（-28.01 vs -974.25）

**结论**：SleepDeceptiveRedAgent成功利用了Cardiff的指纹识别机制缺陷，在Cardiff放松防守后发动突然攻击，造成毁灭性破坏。而我们的模型因实时监控威胁变化，不受前期行为影响，保持了稳定的防御效果。

#### 发现3：防御稳定性

从标准差数据可以观察到：

**DeceptiveRedAgent场景（100步）**：
- 我们的模型：标准差 ±11.56
- Cardiff模型：标准差 ±37.90（是我们的3.3倍）

**SleepDeceptiveRedAgent场景（100步）**：
- 我们的模型：标准差 ±6.95
- Cardiff模型：标准差 ±29.41（是我们的4.2倍）

**结论**：我们的模型不仅平均性能更优，且稳定性显著更好，展现出更强的鲁棒性。

### 4.3 架构差异分析

#### 我们的模型（Selector + MoE）

**决策机制**：
- 基于实时威胁状态计算（threat score）
- 每步动态选择最高威胁子网
- 不依赖对攻击类型的识别

**对新型攻击的适应性**：
- DeceptiveRedAgent：前3步B_line阶段攻击效率较低，反而更容易防御
- SleepDeceptiveRedAgent：实时监控威胁变化，不依赖前期行为判断，第4步威胁出现时立即响应

#### Cardiff模型（PPO + Fingerprinting）

**决策机制**：
- 观察前几步行为，识别攻击类型
- 根据识别结果加载对应的防御模型
- 依赖攻击模式的准确识别

**对新型攻击的挑战**：
- DeceptiveRedAgent：初期识别为B_line，但实际攻击为随机模式
- SleepDeceptiveRedAgent：初期误判为无威胁，防御准备不足

### 4.4 实验意义

#### 学术价值

1. **模式识别的局限性**：验证了基于早期行为模式识别的防御系统在面对模式切换时的脆弱性

2. **实时监控的鲁棒性**：展示了基于实时状态监控的防御架构对攻击模式变化的天然适应能力

3. **架构设计的启示**：提供了网络防御AI系统设计的新思路——关注"威胁在哪"而非"这是什么攻击"

#### 实战参考

1. **APT攻击模拟**：
   - DeceptiveRedAgent模拟多阶段攻击中的战术切换
   - SleepDeceptiveRedAgent模拟长期潜伏后的突然爆发

2. **防御系统评估**：
   - 提供了评估防御系统对未知攻击模式适应能力的测试方法
   - 强调了动态适应性在实战中的重要性

3. **攻防对抗研究**：
   - 展示了欺骗性攻击对基于学习的防御系统的有效性
   - 为防御系统的鲁棒性研究提供了新的测试场景

---

## 5. 总结

### 5.1 核心贡献

1. **设计了两种新型欺骗性红队智能体**：
   - **DeceptiveRedAgent**（指纹欺骗型）：前3步模仿B_line，诱导误判，然后切换攻击模式
   - **SleepDeceptiveRedAgent**（潜伏欺骗型）：前3步完全Sleep，伪装无威胁，然后突然发动全面攻击

2. **验证了基于实时威胁监控架构的优势**：
   - 在DeceptiveRedAgent场景下，防御效果达到99.2%，比Cardiff高4.3%
   - 在SleepDeceptiveRedAgent场景下，防御效果达到97.7%，比Cardiff高79.2%
   - 证明了实时威胁监控架构对欺骗性攻击的强大鲁棒性

3. **揭示了指纹识别机制的脆弱性**：
   - Cardiff在面对模式切换攻击时防御效果下降
   - Cardiff在面对潜伏欺骗攻击时几乎完全失效（仅18.5%防御效果）
   - 为防御系统设计提供了重要的安全性警示

### 5.2 实验数据总结

| 指标 | 我们的模型 | Cardiff模型 | 性能优势 |
|------|-----------|------------|---------|
| DeceptiveRedAgent防御效果（100步） | 99.2% | 94.9% | +4.3% |
| SleepDeceptiveRedAgent防御效果（100步） | 97.7% | 18.5% | +79.2% ✨ |
| DeceptiveRedAgent损失（100步） | -7.74 | -49.59 | **6.4倍** |
| SleepDeceptiveRedAgent损失（100步） | -28.01 | -974.25 | **34.8倍** ✨ |
| DeceptiveRedAgent稳定性（100步） | ±11.56 | ±37.90 | 3.3倍 |
| SleepDeceptiveRedAgent稳定性（100步） | ±6.95 | ±29.41 | 4.2倍 |

*注：✨ 表示显著优势*

### 5.3 结论

通过系统的对比实验，我们验证了不同防御架构在面对新型攻击模式时的表现差异：

**核心发现**：
1. **DeceptiveRedAgent场景**：两个模型均保持良好防御效果，我们的模型略优（99.2% vs 94.9%）
2. **SleepDeceptiveRedAgent场景**：性能差距显著扩大，我们的模型优势达到**34.8倍**（-28.01 vs -974.25）
3. **架构差异的影响**：Cardiff的指纹识别机制在面对欺骗性攻击时存在致命弱点，而我们基于实时威胁监控的架构展现出强大的鲁棒性

**实验结果表明**，基于实时威胁监控的防御架构在应对攻击模式变化时展现出更好的适应性和稳定性，特别是在面对利用指纹识别缺陷的欺骗性攻击时，优势尤为明显。这为网络防御AI系统的设计提供了有价值的参考。

---


